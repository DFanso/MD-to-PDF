# Lesson 03 - Interfaces and Interaction Paradigms

1. Evaluation: Why, What, Where, and When
   - Continuous iterative design and evaluation process
   - Why: Check users' requirements, usability, and satisfaction
   - What: Conceptual models, prototypes, and competitive products
   - Where: Natural settings, labs, and in-the-wild
   - When: Throughout the design process and for finished products

2. Types of Evaluation
   - Controlled settings directly involving users (most popular)
   - Natural settings involving users (gaining popularity)
   - Settings not directly involving users (less common)

3. Living Labs
   - Evaluate people's use of technology in everyday lives
   - Examples: Aware Home, EU-funded research projects in cities

4. Evaluation Case Studies
   - Experimental investigation of physiological responses in computer games
   - Ethnographic study of visitors at the Royal Highland show using a mobile app
   - Crowdsourcing opinions and reactions to inform technology evaluation

5. Participants' Rights and Informed Consent
   - Participants must be informed about the evaluation purpose, process, and their rights
   - Informed consent forms act as a contract between participants and researchers
   - Evaluation design, data analysis, and storage methods require ethical approval

6. Interpreting Data
   - Consider reliability, validity, ecological validity, biases, and scope of the results

7. Usability Testing
   - Involves recording performance of typical users doing typical tasks
   - Conducted in controlled settings with 5-10 representative users
   - Quantitative performance measures include success rates, time, errors, and satisfaction

8. Experiments
   - Test hypotheses about relationships between variables (independent and dependent)
   - Different experimental designs: between-subjects, within-subjects, and matched-pairs
   - Must be replicable and validated statistically

9. Field Studies
   - Conducted in natural settings to understand user behavior and technology impact
   - Used to identify opportunities, determine requirements, and evaluate technology in use
   - Example: Painpad study in hospitals to evaluate a pain-monitoring device

10. Inspections (Evaluation without Users)
    - Expert reviews (formal or informal) based on knowledge of users and technology
    - Heuristic evaluation guided by a set of usability heuristics (e.g., Nielsen's heuristics)
    - Cognitive walkthroughs focusing on ease of learning through usage scenarios

11. Web Analytics and A/B Testing
    - Web analytics: Analyzing user activities on websites to inform design improvements
    - A/B testing: Large-scale experiments comparing user performance on two design versions

12. Predictive Models
    - Evaluating products or designs without directly involving users
    - Based on expert error-free behavior (e.g., Fitts' Law for pointing time prediction)
